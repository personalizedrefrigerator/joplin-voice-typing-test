{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Updating Whisper's default vocabulary\n",
        "\n",
        "This notebook removes certain tokens from the default Whisper vocabulary.\n",
        "\n",
        "**Goal**: Prevent `whisper-tiny` from occasionally outputting English profanity when given noisy input with no speech."
      ],
      "metadata": {
        "id": "oIPd30CuphDD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6J7tG1nfpeQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62f2c3b-e019-4490-dff0-e40bdc7f19c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: datasets[audio] in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets[audio]) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (6.0.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.13.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.10.2.post1)\n",
            "Requirement already satisfied: soxr>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from datasets[audio]) (0.5.0.post1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.12.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[audio]) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[audio]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[audio]) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[audio]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[audio]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[audio]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[audio]) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets[audio]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets[audio]) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->datasets[audio]) (1.17.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (0.61.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.8.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->datasets[audio]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[audio]) (2025.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->datasets[audio]) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "# jiwer is used for the word error rate (WER) metric\n",
        "!pip install --upgrade datasets[audio] transformers evaluate jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# See https://discuss.huggingface.co/t/how-to-turn-wandb-off-in-trainer/6237/10\n",
        "wandb.init(mode='disabled')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "uCkfg9oqhW97",
        "outputId": "4e4be868-b66c-43dd-88ff-4ace7f6c03aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/boxs3yl4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7dacd4c60e90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "checkpoint_path = Path('./whisper/checkpoints').resolve()"
      ],
      "metadata": {
        "id": "hmdwvSfAmPwo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n"
      ],
      "metadata": {
        "id": "h24UgNf7mVOB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the feature extractor and tokenizer\n",
        "\n",
        "We'll be fine-tuning the `openai/whisper-tiny` model. Here, the feature extractor and tokenizer for this model are fetched from Huggingface:"
      ],
      "metadata": {
        "id": "M1HyDz2cRvVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
        "\n",
        "finetune_from_id = 'openai/whisper-tiny'\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(finetune_from_id,  task='transcribe')\n",
        "tokenizer_original = WhisperTokenizer.from_pretrained(finetune_from_id, task='transcribe')"
      ],
      "metadata": {
        "id": "FPGYcxa7zmO5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a customized tokenizer based on `tokenizer_original` in the next section."
      ],
      "metadata": {
        "id": "dAjHVQPCueTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary adjustements\n",
        "\n",
        "Next, we remove several unwanted tokens from the vocabulary:"
      ],
      "metadata": {
        "id": "S2-3B1TW3Tc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Save the vocabulary to a file\n",
        "tokenizer_directory = Path('whisper-default-tokenizer')\n",
        "tokenizer_original.save_pretrained(tokenizer_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWsP8YfrcmUC",
        "outputId": "24c78e8f-ace2-4d74-857b-5bd04cebfe17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('whisper-default-tokenizer/tokenizer_config.json',\n",
              " 'whisper-default-tokenizer/special_tokens_map.json',\n",
              " 'whisper-default-tokenizer/vocab.json',\n",
              " 'whisper-default-tokenizer/merges.txt',\n",
              " 'whisper-default-tokenizer/normalizer.json',\n",
              " 'whisper-default-tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the tokenizer is saved in `tokenizer_directory`, we can load `tokenizer_directory/vocab.json` and modify it:"
      ],
      "metadata": {
        "id": "ES0enoY-ezxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Get vocab.json\n",
        "import json\n",
        "\n",
        "def json_from_path(path: Path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return json.loads(f.read())\n",
        "\n",
        "vocab = json_from_path(tokenizer_directory / 'vocab.json')"
      ],
      "metadata": {
        "id": "jtHNBGVPewT2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "NONWORD_REGEX = re.compile(r'[ \\t?.,;!()/\\-«»]+')\n",
        "def split_by_word(text: str):\n",
        "    \"\"\" Splits the given `text` into words. Returns a list of those words. \"\"\"\n",
        "    return NONWORD_REGEX.split(text)\n",
        "\n",
        "\n",
        "# This character marks the beginning of a word in vocab.json\n",
        "word_start_char = 'Ġ'"
      ],
      "metadata": {
        "id": "TBqPdNpLkSYg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Replace!\n",
        "next_replacement_idx = 0\n",
        "new_vocab = {}\n",
        "\n",
        "# Token IDs can be found by inspecting the original vocab.json\n",
        "token_id_remappings = {\n",
        "    22676: word_start_char + \"BS\",\n",
        "    19186: \"s***\",\n",
        "    30748: word_start_char + \"s*****\",\n",
        "    4611: word_start_char + \"s***\",\n",
        "    19593: word_start_char + \"S***\",\n",
        "    10965: word_start_char + \"F***\",\n",
        "    26154: word_start_char + \"F!!!\",\n",
        "    33342: word_start_char + \"F******\",\n",
        "    47069: word_start_char + \"f***(0)\",\n",
        "    3275: word_start_char + \"f***(1)\",\n",
        "    22518: word_start_char + \"f***(2)\",\n",
        "    20022: word_start_char + \"f***(3)\",\n",
        "    5546: word_start_char + \"f******\",\n",
        "    47069: word_start_char + \"m****(1)\",\n",
        "    29537: word_start_char + \"m****(2)\",\n",
        "    11960: word_start_char + \"b****\",\n",
        "    42094: word_start_char + \"b******\",\n",
        "    40678: word_start_char + \"B****\"\n",
        "}\n",
        "replaced_keys = set()\n",
        "\n",
        "for key in vocab:\n",
        "    token_id = vocab[key]\n",
        "    if token_id in token_id_remappings:\n",
        "        new_key = token_id_remappings[token_id]\n",
        "        new_vocab[new_key] = token_id\n",
        "        replaced_keys.add(key)\n",
        "    else:\n",
        "        new_vocab[key] = token_id\n",
        "\n",
        "new_merges = []\n",
        "with open(tokenizer_directory / 'merges.txt', 'r', encoding='utf-8') as merges:\n",
        "    for line in merges.readlines():\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        words = split_by_word(line)\n",
        "        if not (words[0] in replaced_keys):\n",
        "            new_merges.append(line.strip())"
      ],
      "metadata": {
        "id": "PPBQrfHmsEdz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check for other indexes to replace (keeping in mind that the output should still be multi-lingual), we could do something like this:\n",
        "```python\n",
        "!pip install better_profanity==0.7.0\n",
        "\n",
        "from better_profanity import profanity\n",
        "\n",
        "profanity.load_censor_words()\n",
        "for key in new_vocab:\n",
        "    word = key\n",
        "    if key.startswith(word_start_char):\n",
        "        word = key[1:]\n",
        "    if profanity.contains_profanity(word):\n",
        "        print(\"Consider replacing\", key, new_vocab[key])\n",
        "```"
      ],
      "metadata": {
        "id": "EKCuKjtPIRaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We now have an updated vocab file!"
      ],
      "metadata": {
        "id": "y_lDgRsIuLRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write to a file\n",
        "tokenizer_fr_directory = Path('updated-tokenizer')\n",
        "if tokenizer_fr_directory.exists():\n",
        "    shutil.rmtree(tokenizer_fr_directory)\n",
        "shutil.copytree(tokenizer_directory, tokenizer_fr_directory)\n",
        "with open(tokenizer_fr_directory / 'vocab.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(new_vocab, f, ensure_ascii=False)\n",
        "\n",
        "\n",
        "with open(tokenizer_fr_directory / 'merges.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(new_merges))"
      ],
      "metadata": {
        "id": "xa4M3M_02GwI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "# Use a normal WhisperTokenizer -- WhisperTokenizerFast has trouble with the updated\n",
        "# vocabulary.\n",
        "tokenizer = WhisperTokenizer(\n",
        "    tokenizer_fr_directory / 'vocab.json',\n",
        "    tokenizer_fr_directory / 'merges.txt',\n",
        "    tokenizer_fr_directory / 'normalizer.json',\n",
        "    bos_token='<|startoftranscript|>',\n",
        "    unk_token='',\n",
        "    pad_token='<|endoftext|>',\n",
        ")\n",
        "\n",
        "# See https://discuss.huggingface.co/t/fine-tuning-whisper-on-my-own-dataset-with-a-customized-tokenizer/25903\n",
        "tokenizer.add_special_tokens(tokenizer_original.special_tokens_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oSULQLZz5rQ",
        "outputId": "99a2c07f-4036-4e18-bffb-15a84d802ec0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For debugging, update the output directory\n",
        "shutil.rmtree(tokenizer_fr_directory)\n",
        "tokenizer.save_pretrained(tokenizer_fr_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl_cU43c7IHR",
        "outputId": "7468d346-41d9-4abb-cf87-82c00979e251"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('updated-tokenizer/tokenizer_config.json',\n",
              " 'updated-tokenizer/special_tokens_map.json',\n",
              " 'updated-tokenizer/vocab.json',\n",
              " 'updated-tokenizer/merges.txt',\n",
              " 'updated-tokenizer/normalizer.json',\n",
              " 'updated-tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the processor\n",
        "\n",
        "Next, load the `WhisperProcessor`, which combines a feature extractor and tokenizer."
      ],
      "metadata": {
        "id": "DEcid4Cj4H_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor(feature_extractor, tokenizer)"
      ],
      "metadata": {
        "id": "3usf8ghTRHrW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, build the model:"
      ],
      "metadata": {
        "id": "QMpuESBPSRs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(finetune_from_id)\n",
        "model.generation_config.forced_decoder_ids = None\n"
      ],
      "metadata": {
        "id": "X6EHVfw4TRLz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "import torch\n",
        "# See the linked blog post and https://huggingface.co/docs/transformers/main_classes/data_collator\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorWithPadding:\n",
        "    ''' Converts raw data into a batch ready for the model '''\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: list) -> dict[str, torch.Tensor]:\n",
        "        input_features = [{'input_features': f['input_features']} for f in features]\n",
        "        label_features = [{'input_ids': f['labels']} for f in features]\n",
        "\n",
        "        # According to the linked blog post, the input and label features need\n",
        "        # to be padded separately (due to different final lengths), then\n",
        "        # recombined:\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt')\n",
        "\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors='pt')\n",
        "\n",
        "        # transformers uses -100 for masking\n",
        "        labels = labels_batch['input_ids'].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # Don't double-prepend the beginning of sequence token:\n",
        "        if (labels[:,0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch['labels'] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorWithPadding(processor=processor, decoder_start_token_id=model.config.decoder_start_token_id)"
      ],
      "metadata": {
        "id": "JVAJV11oUs5A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output_dir = Path('./final-model').resolve()\n",
        "model.save_pretrained(model_output_dir)\n",
        "tokenizer.save_pretrained(model_output_dir)"
      ],
      "metadata": {
        "id": "jAjMqolA-957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c5069d1-27ca-4d44-babe-2e8089036101"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/final-model/tokenizer_config.json',\n",
              " '/content/final-model/special_tokens_map.json',\n",
              " '/content/final-model/vocab.json',\n",
              " '/content/final-model/merges.txt',\n",
              " '/content/final-model/normalizer.json',\n",
              " '/content/final-model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model conversion\n",
        "\n",
        "Next, we need to convert the model into a format usable by Joplin. This next step converts the model from PyTorch to GGML."
      ],
      "metadata": {
        "id": "g0e9GSFSfDsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/whisper whisper-github\n",
        "!git clone https://github.com/ggerganov/whisper.cpp\n",
        "!cd whisper.cpp && git checkout v1.7.4"
      ],
      "metadata": {
        "id": "Sz5y6PdM5Qei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867a77b7-3d5e-4b9b-956d-bb0ee538f6ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'whisper-github' already exists and is not an empty directory.\n",
            "fatal: destination path 'whisper.cpp' already exists and is not an empty directory.\n",
            "M\tmodels/convert-h5-to-ggml.py\n",
            "HEAD is now at 8a9ad78 release : v1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch convert-h5-to-ggml to work with more recent model versions\n",
        "conversion_script_path = Path('whisper.cpp/models/convert-h5-to-ggml.py')\n",
        "conversion_script_content = conversion_script_path.read_text()\n",
        "with open(conversion_script_path, 'w') as conversion_script:\n",
        "    bad_if_statement = 'if \"max_length\" not in hparams:'\n",
        "    replaced_if_statement = 'if \"max_length\" not in hparams or hparams[\"max_length\"] == None:'\n",
        "    conversion_script.write(conversion_script_content.replace(bad_if_statement, replaced_if_statement))"
      ],
      "metadata": {
        "id": "MUMzKNCmJfq0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./ggml\n",
        "!python whisper.cpp/models/convert-h5-to-ggml.py ./final-model ./whisper-github ./ggml\n",
        "!mv ./ggml/ggml-model.bin ./ggml/ggml-clean.bin"
      ],
      "metadata": {
        "id": "A6eeyWNgdw8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2457eab9-1daf-4c41-fa42-26b24dde96bc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./ggml’: File exists\n",
            "2025-02-26 16:43:14.799406: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1740588194.849513   14409 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1740588194.864821   14409 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "model.encoder.conv1.weight  ->  encoder.conv1.weight\n",
            "encoder.conv1.weight 3 (384, 80, 3)\n",
            "model.encoder.conv1.bias  ->  encoder.conv1.bias\n",
            "  Reshaped variable:  encoder.conv1.bias  to shape:  (384, 1)\n",
            "encoder.conv1.bias 2 (384, 1)\n",
            "  Converting to float32\n",
            "model.encoder.conv2.weight  ->  encoder.conv2.weight\n",
            "encoder.conv2.weight 3 (384, 384, 3)\n",
            "model.encoder.conv2.bias  ->  encoder.conv2.bias\n",
            "  Reshaped variable:  encoder.conv2.bias  to shape:  (384, 1)\n",
            "encoder.conv2.bias 2 (384, 1)\n",
            "  Converting to float32\n",
            "model.encoder.embed_positions.weight  ->  encoder.positional_embedding\n",
            "encoder.positional_embedding 2 (1500, 384)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn.k_proj.weight  ->  encoder.blocks.0.attn.key.weight\n",
            "encoder.blocks.0.attn.key.weight 2 (384, 384)\n",
            "model.encoder.layers.0.self_attn.v_proj.weight  ->  encoder.blocks.0.attn.value.weight\n",
            "encoder.blocks.0.attn.value.weight 2 (384, 384)\n",
            "model.encoder.layers.0.self_attn.v_proj.bias  ->  encoder.blocks.0.attn.value.bias\n",
            "encoder.blocks.0.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn.q_proj.weight  ->  encoder.blocks.0.attn.query.weight\n",
            "encoder.blocks.0.attn.query.weight 2 (384, 384)\n",
            "model.encoder.layers.0.self_attn.q_proj.bias  ->  encoder.blocks.0.attn.query.bias\n",
            "encoder.blocks.0.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn.out_proj.weight  ->  encoder.blocks.0.attn.out.weight\n",
            "encoder.blocks.0.attn.out.weight 2 (384, 384)\n",
            "model.encoder.layers.0.self_attn.out_proj.bias  ->  encoder.blocks.0.attn.out.bias\n",
            "encoder.blocks.0.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn_layer_norm.weight  ->  encoder.blocks.0.attn_ln.weight\n",
            "encoder.blocks.0.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.self_attn_layer_norm.bias  ->  encoder.blocks.0.attn_ln.bias\n",
            "encoder.blocks.0.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.fc1.weight  ->  encoder.blocks.0.mlp.0.weight\n",
            "encoder.blocks.0.mlp.0.weight 2 (1536, 384)\n",
            "model.encoder.layers.0.fc1.bias  ->  encoder.blocks.0.mlp.0.bias\n",
            "encoder.blocks.0.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.fc2.weight  ->  encoder.blocks.0.mlp.2.weight\n",
            "encoder.blocks.0.mlp.2.weight 2 (384, 1536)\n",
            "model.encoder.layers.0.fc2.bias  ->  encoder.blocks.0.mlp.2.bias\n",
            "encoder.blocks.0.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.final_layer_norm.weight  ->  encoder.blocks.0.mlp_ln.weight\n",
            "encoder.blocks.0.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.0.final_layer_norm.bias  ->  encoder.blocks.0.mlp_ln.bias\n",
            "encoder.blocks.0.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn.k_proj.weight  ->  encoder.blocks.1.attn.key.weight\n",
            "encoder.blocks.1.attn.key.weight 2 (384, 384)\n",
            "model.encoder.layers.1.self_attn.v_proj.weight  ->  encoder.blocks.1.attn.value.weight\n",
            "encoder.blocks.1.attn.value.weight 2 (384, 384)\n",
            "model.encoder.layers.1.self_attn.v_proj.bias  ->  encoder.blocks.1.attn.value.bias\n",
            "encoder.blocks.1.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn.q_proj.weight  ->  encoder.blocks.1.attn.query.weight\n",
            "encoder.blocks.1.attn.query.weight 2 (384, 384)\n",
            "model.encoder.layers.1.self_attn.q_proj.bias  ->  encoder.blocks.1.attn.query.bias\n",
            "encoder.blocks.1.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn.out_proj.weight  ->  encoder.blocks.1.attn.out.weight\n",
            "encoder.blocks.1.attn.out.weight 2 (384, 384)\n",
            "model.encoder.layers.1.self_attn.out_proj.bias  ->  encoder.blocks.1.attn.out.bias\n",
            "encoder.blocks.1.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn_layer_norm.weight  ->  encoder.blocks.1.attn_ln.weight\n",
            "encoder.blocks.1.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.self_attn_layer_norm.bias  ->  encoder.blocks.1.attn_ln.bias\n",
            "encoder.blocks.1.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.fc1.weight  ->  encoder.blocks.1.mlp.0.weight\n",
            "encoder.blocks.1.mlp.0.weight 2 (1536, 384)\n",
            "model.encoder.layers.1.fc1.bias  ->  encoder.blocks.1.mlp.0.bias\n",
            "encoder.blocks.1.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.fc2.weight  ->  encoder.blocks.1.mlp.2.weight\n",
            "encoder.blocks.1.mlp.2.weight 2 (384, 1536)\n",
            "model.encoder.layers.1.fc2.bias  ->  encoder.blocks.1.mlp.2.bias\n",
            "encoder.blocks.1.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.final_layer_norm.weight  ->  encoder.blocks.1.mlp_ln.weight\n",
            "encoder.blocks.1.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.1.final_layer_norm.bias  ->  encoder.blocks.1.mlp_ln.bias\n",
            "encoder.blocks.1.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn.k_proj.weight  ->  encoder.blocks.2.attn.key.weight\n",
            "encoder.blocks.2.attn.key.weight 2 (384, 384)\n",
            "model.encoder.layers.2.self_attn.v_proj.weight  ->  encoder.blocks.2.attn.value.weight\n",
            "encoder.blocks.2.attn.value.weight 2 (384, 384)\n",
            "model.encoder.layers.2.self_attn.v_proj.bias  ->  encoder.blocks.2.attn.value.bias\n",
            "encoder.blocks.2.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn.q_proj.weight  ->  encoder.blocks.2.attn.query.weight\n",
            "encoder.blocks.2.attn.query.weight 2 (384, 384)\n",
            "model.encoder.layers.2.self_attn.q_proj.bias  ->  encoder.blocks.2.attn.query.bias\n",
            "encoder.blocks.2.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn.out_proj.weight  ->  encoder.blocks.2.attn.out.weight\n",
            "encoder.blocks.2.attn.out.weight 2 (384, 384)\n",
            "model.encoder.layers.2.self_attn.out_proj.bias  ->  encoder.blocks.2.attn.out.bias\n",
            "encoder.blocks.2.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn_layer_norm.weight  ->  encoder.blocks.2.attn_ln.weight\n",
            "encoder.blocks.2.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.self_attn_layer_norm.bias  ->  encoder.blocks.2.attn_ln.bias\n",
            "encoder.blocks.2.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.fc1.weight  ->  encoder.blocks.2.mlp.0.weight\n",
            "encoder.blocks.2.mlp.0.weight 2 (1536, 384)\n",
            "model.encoder.layers.2.fc1.bias  ->  encoder.blocks.2.mlp.0.bias\n",
            "encoder.blocks.2.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.fc2.weight  ->  encoder.blocks.2.mlp.2.weight\n",
            "encoder.blocks.2.mlp.2.weight 2 (384, 1536)\n",
            "model.encoder.layers.2.fc2.bias  ->  encoder.blocks.2.mlp.2.bias\n",
            "encoder.blocks.2.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.final_layer_norm.weight  ->  encoder.blocks.2.mlp_ln.weight\n",
            "encoder.blocks.2.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.2.final_layer_norm.bias  ->  encoder.blocks.2.mlp_ln.bias\n",
            "encoder.blocks.2.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn.k_proj.weight  ->  encoder.blocks.3.attn.key.weight\n",
            "encoder.blocks.3.attn.key.weight 2 (384, 384)\n",
            "model.encoder.layers.3.self_attn.v_proj.weight  ->  encoder.blocks.3.attn.value.weight\n",
            "encoder.blocks.3.attn.value.weight 2 (384, 384)\n",
            "model.encoder.layers.3.self_attn.v_proj.bias  ->  encoder.blocks.3.attn.value.bias\n",
            "encoder.blocks.3.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn.q_proj.weight  ->  encoder.blocks.3.attn.query.weight\n",
            "encoder.blocks.3.attn.query.weight 2 (384, 384)\n",
            "model.encoder.layers.3.self_attn.q_proj.bias  ->  encoder.blocks.3.attn.query.bias\n",
            "encoder.blocks.3.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn.out_proj.weight  ->  encoder.blocks.3.attn.out.weight\n",
            "encoder.blocks.3.attn.out.weight 2 (384, 384)\n",
            "model.encoder.layers.3.self_attn.out_proj.bias  ->  encoder.blocks.3.attn.out.bias\n",
            "encoder.blocks.3.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn_layer_norm.weight  ->  encoder.blocks.3.attn_ln.weight\n",
            "encoder.blocks.3.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.self_attn_layer_norm.bias  ->  encoder.blocks.3.attn_ln.bias\n",
            "encoder.blocks.3.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.fc1.weight  ->  encoder.blocks.3.mlp.0.weight\n",
            "encoder.blocks.3.mlp.0.weight 2 (1536, 384)\n",
            "model.encoder.layers.3.fc1.bias  ->  encoder.blocks.3.mlp.0.bias\n",
            "encoder.blocks.3.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.fc2.weight  ->  encoder.blocks.3.mlp.2.weight\n",
            "encoder.blocks.3.mlp.2.weight 2 (384, 1536)\n",
            "model.encoder.layers.3.fc2.bias  ->  encoder.blocks.3.mlp.2.bias\n",
            "encoder.blocks.3.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.final_layer_norm.weight  ->  encoder.blocks.3.mlp_ln.weight\n",
            "encoder.blocks.3.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layers.3.final_layer_norm.bias  ->  encoder.blocks.3.mlp_ln.bias\n",
            "encoder.blocks.3.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layer_norm.weight  ->  encoder.ln_post.weight\n",
            "encoder.ln_post.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.encoder.layer_norm.bias  ->  encoder.ln_post.bias\n",
            "encoder.ln_post.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.embed_tokens.weight  ->  decoder.token_embedding.weight\n",
            "decoder.token_embedding.weight 2 (51865, 384)\n",
            "model.decoder.embed_positions.weight  ->  decoder.positional_embedding\n",
            "decoder.positional_embedding 2 (448, 384)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn.k_proj.weight  ->  decoder.blocks.0.attn.key.weight\n",
            "decoder.blocks.0.attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.0.self_attn.v_proj.weight  ->  decoder.blocks.0.attn.value.weight\n",
            "decoder.blocks.0.attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.0.self_attn.v_proj.bias  ->  decoder.blocks.0.attn.value.bias\n",
            "decoder.blocks.0.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn.q_proj.weight  ->  decoder.blocks.0.attn.query.weight\n",
            "decoder.blocks.0.attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.0.self_attn.q_proj.bias  ->  decoder.blocks.0.attn.query.bias\n",
            "decoder.blocks.0.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn.out_proj.weight  ->  decoder.blocks.0.attn.out.weight\n",
            "decoder.blocks.0.attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.0.self_attn.out_proj.bias  ->  decoder.blocks.0.attn.out.bias\n",
            "decoder.blocks.0.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn_layer_norm.weight  ->  decoder.blocks.0.attn_ln.weight\n",
            "decoder.blocks.0.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.self_attn_layer_norm.bias  ->  decoder.blocks.0.attn_ln.bias\n",
            "decoder.blocks.0.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn.k_proj.weight  ->  decoder.blocks.0.cross_attn.key.weight\n",
            "decoder.blocks.0.cross_attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.0.encoder_attn.v_proj.weight  ->  decoder.blocks.0.cross_attn.value.weight\n",
            "decoder.blocks.0.cross_attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.0.encoder_attn.v_proj.bias  ->  decoder.blocks.0.cross_attn.value.bias\n",
            "decoder.blocks.0.cross_attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn.q_proj.weight  ->  decoder.blocks.0.cross_attn.query.weight\n",
            "decoder.blocks.0.cross_attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.0.encoder_attn.q_proj.bias  ->  decoder.blocks.0.cross_attn.query.bias\n",
            "decoder.blocks.0.cross_attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn.out_proj.weight  ->  decoder.blocks.0.cross_attn.out.weight\n",
            "decoder.blocks.0.cross_attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.0.encoder_attn.out_proj.bias  ->  decoder.blocks.0.cross_attn.out.bias\n",
            "decoder.blocks.0.cross_attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn_layer_norm.weight  ->  decoder.blocks.0.cross_attn_ln.weight\n",
            "decoder.blocks.0.cross_attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.encoder_attn_layer_norm.bias  ->  decoder.blocks.0.cross_attn_ln.bias\n",
            "decoder.blocks.0.cross_attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.fc1.weight  ->  decoder.blocks.0.mlp.0.weight\n",
            "decoder.blocks.0.mlp.0.weight 2 (1536, 384)\n",
            "model.decoder.layers.0.fc1.bias  ->  decoder.blocks.0.mlp.0.bias\n",
            "decoder.blocks.0.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.fc2.weight  ->  decoder.blocks.0.mlp.2.weight\n",
            "decoder.blocks.0.mlp.2.weight 2 (384, 1536)\n",
            "model.decoder.layers.0.fc2.bias  ->  decoder.blocks.0.mlp.2.bias\n",
            "decoder.blocks.0.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.final_layer_norm.weight  ->  decoder.blocks.0.mlp_ln.weight\n",
            "decoder.blocks.0.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.0.final_layer_norm.bias  ->  decoder.blocks.0.mlp_ln.bias\n",
            "decoder.blocks.0.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn.k_proj.weight  ->  decoder.blocks.1.attn.key.weight\n",
            "decoder.blocks.1.attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.1.self_attn.v_proj.weight  ->  decoder.blocks.1.attn.value.weight\n",
            "decoder.blocks.1.attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.1.self_attn.v_proj.bias  ->  decoder.blocks.1.attn.value.bias\n",
            "decoder.blocks.1.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn.q_proj.weight  ->  decoder.blocks.1.attn.query.weight\n",
            "decoder.blocks.1.attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.1.self_attn.q_proj.bias  ->  decoder.blocks.1.attn.query.bias\n",
            "decoder.blocks.1.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn.out_proj.weight  ->  decoder.blocks.1.attn.out.weight\n",
            "decoder.blocks.1.attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.1.self_attn.out_proj.bias  ->  decoder.blocks.1.attn.out.bias\n",
            "decoder.blocks.1.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn_layer_norm.weight  ->  decoder.blocks.1.attn_ln.weight\n",
            "decoder.blocks.1.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.self_attn_layer_norm.bias  ->  decoder.blocks.1.attn_ln.bias\n",
            "decoder.blocks.1.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn.k_proj.weight  ->  decoder.blocks.1.cross_attn.key.weight\n",
            "decoder.blocks.1.cross_attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.1.encoder_attn.v_proj.weight  ->  decoder.blocks.1.cross_attn.value.weight\n",
            "decoder.blocks.1.cross_attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.1.encoder_attn.v_proj.bias  ->  decoder.blocks.1.cross_attn.value.bias\n",
            "decoder.blocks.1.cross_attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn.q_proj.weight  ->  decoder.blocks.1.cross_attn.query.weight\n",
            "decoder.blocks.1.cross_attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.1.encoder_attn.q_proj.bias  ->  decoder.blocks.1.cross_attn.query.bias\n",
            "decoder.blocks.1.cross_attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn.out_proj.weight  ->  decoder.blocks.1.cross_attn.out.weight\n",
            "decoder.blocks.1.cross_attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.1.encoder_attn.out_proj.bias  ->  decoder.blocks.1.cross_attn.out.bias\n",
            "decoder.blocks.1.cross_attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn_layer_norm.weight  ->  decoder.blocks.1.cross_attn_ln.weight\n",
            "decoder.blocks.1.cross_attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.encoder_attn_layer_norm.bias  ->  decoder.blocks.1.cross_attn_ln.bias\n",
            "decoder.blocks.1.cross_attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.fc1.weight  ->  decoder.blocks.1.mlp.0.weight\n",
            "decoder.blocks.1.mlp.0.weight 2 (1536, 384)\n",
            "model.decoder.layers.1.fc1.bias  ->  decoder.blocks.1.mlp.0.bias\n",
            "decoder.blocks.1.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.fc2.weight  ->  decoder.blocks.1.mlp.2.weight\n",
            "decoder.blocks.1.mlp.2.weight 2 (384, 1536)\n",
            "model.decoder.layers.1.fc2.bias  ->  decoder.blocks.1.mlp.2.bias\n",
            "decoder.blocks.1.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.final_layer_norm.weight  ->  decoder.blocks.1.mlp_ln.weight\n",
            "decoder.blocks.1.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.1.final_layer_norm.bias  ->  decoder.blocks.1.mlp_ln.bias\n",
            "decoder.blocks.1.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn.k_proj.weight  ->  decoder.blocks.2.attn.key.weight\n",
            "decoder.blocks.2.attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.2.self_attn.v_proj.weight  ->  decoder.blocks.2.attn.value.weight\n",
            "decoder.blocks.2.attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.2.self_attn.v_proj.bias  ->  decoder.blocks.2.attn.value.bias\n",
            "decoder.blocks.2.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn.q_proj.weight  ->  decoder.blocks.2.attn.query.weight\n",
            "decoder.blocks.2.attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.2.self_attn.q_proj.bias  ->  decoder.blocks.2.attn.query.bias\n",
            "decoder.blocks.2.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn.out_proj.weight  ->  decoder.blocks.2.attn.out.weight\n",
            "decoder.blocks.2.attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.2.self_attn.out_proj.bias  ->  decoder.blocks.2.attn.out.bias\n",
            "decoder.blocks.2.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn_layer_norm.weight  ->  decoder.blocks.2.attn_ln.weight\n",
            "decoder.blocks.2.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.self_attn_layer_norm.bias  ->  decoder.blocks.2.attn_ln.bias\n",
            "decoder.blocks.2.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn.k_proj.weight  ->  decoder.blocks.2.cross_attn.key.weight\n",
            "decoder.blocks.2.cross_attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.2.encoder_attn.v_proj.weight  ->  decoder.blocks.2.cross_attn.value.weight\n",
            "decoder.blocks.2.cross_attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.2.encoder_attn.v_proj.bias  ->  decoder.blocks.2.cross_attn.value.bias\n",
            "decoder.blocks.2.cross_attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn.q_proj.weight  ->  decoder.blocks.2.cross_attn.query.weight\n",
            "decoder.blocks.2.cross_attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.2.encoder_attn.q_proj.bias  ->  decoder.blocks.2.cross_attn.query.bias\n",
            "decoder.blocks.2.cross_attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn.out_proj.weight  ->  decoder.blocks.2.cross_attn.out.weight\n",
            "decoder.blocks.2.cross_attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.2.encoder_attn.out_proj.bias  ->  decoder.blocks.2.cross_attn.out.bias\n",
            "decoder.blocks.2.cross_attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn_layer_norm.weight  ->  decoder.blocks.2.cross_attn_ln.weight\n",
            "decoder.blocks.2.cross_attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.encoder_attn_layer_norm.bias  ->  decoder.blocks.2.cross_attn_ln.bias\n",
            "decoder.blocks.2.cross_attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.fc1.weight  ->  decoder.blocks.2.mlp.0.weight\n",
            "decoder.blocks.2.mlp.0.weight 2 (1536, 384)\n",
            "model.decoder.layers.2.fc1.bias  ->  decoder.blocks.2.mlp.0.bias\n",
            "decoder.blocks.2.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.fc2.weight  ->  decoder.blocks.2.mlp.2.weight\n",
            "decoder.blocks.2.mlp.2.weight 2 (384, 1536)\n",
            "model.decoder.layers.2.fc2.bias  ->  decoder.blocks.2.mlp.2.bias\n",
            "decoder.blocks.2.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.final_layer_norm.weight  ->  decoder.blocks.2.mlp_ln.weight\n",
            "decoder.blocks.2.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.2.final_layer_norm.bias  ->  decoder.blocks.2.mlp_ln.bias\n",
            "decoder.blocks.2.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn.k_proj.weight  ->  decoder.blocks.3.attn.key.weight\n",
            "decoder.blocks.3.attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.3.self_attn.v_proj.weight  ->  decoder.blocks.3.attn.value.weight\n",
            "decoder.blocks.3.attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.3.self_attn.v_proj.bias  ->  decoder.blocks.3.attn.value.bias\n",
            "decoder.blocks.3.attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn.q_proj.weight  ->  decoder.blocks.3.attn.query.weight\n",
            "decoder.blocks.3.attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.3.self_attn.q_proj.bias  ->  decoder.blocks.3.attn.query.bias\n",
            "decoder.blocks.3.attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn.out_proj.weight  ->  decoder.blocks.3.attn.out.weight\n",
            "decoder.blocks.3.attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.3.self_attn.out_proj.bias  ->  decoder.blocks.3.attn.out.bias\n",
            "decoder.blocks.3.attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn_layer_norm.weight  ->  decoder.blocks.3.attn_ln.weight\n",
            "decoder.blocks.3.attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.self_attn_layer_norm.bias  ->  decoder.blocks.3.attn_ln.bias\n",
            "decoder.blocks.3.attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn.k_proj.weight  ->  decoder.blocks.3.cross_attn.key.weight\n",
            "decoder.blocks.3.cross_attn.key.weight 2 (384, 384)\n",
            "model.decoder.layers.3.encoder_attn.v_proj.weight  ->  decoder.blocks.3.cross_attn.value.weight\n",
            "decoder.blocks.3.cross_attn.value.weight 2 (384, 384)\n",
            "model.decoder.layers.3.encoder_attn.v_proj.bias  ->  decoder.blocks.3.cross_attn.value.bias\n",
            "decoder.blocks.3.cross_attn.value.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn.q_proj.weight  ->  decoder.blocks.3.cross_attn.query.weight\n",
            "decoder.blocks.3.cross_attn.query.weight 2 (384, 384)\n",
            "model.decoder.layers.3.encoder_attn.q_proj.bias  ->  decoder.blocks.3.cross_attn.query.bias\n",
            "decoder.blocks.3.cross_attn.query.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn.out_proj.weight  ->  decoder.blocks.3.cross_attn.out.weight\n",
            "decoder.blocks.3.cross_attn.out.weight 2 (384, 384)\n",
            "model.decoder.layers.3.encoder_attn.out_proj.bias  ->  decoder.blocks.3.cross_attn.out.bias\n",
            "decoder.blocks.3.cross_attn.out.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn_layer_norm.weight  ->  decoder.blocks.3.cross_attn_ln.weight\n",
            "decoder.blocks.3.cross_attn_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.encoder_attn_layer_norm.bias  ->  decoder.blocks.3.cross_attn_ln.bias\n",
            "decoder.blocks.3.cross_attn_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.fc1.weight  ->  decoder.blocks.3.mlp.0.weight\n",
            "decoder.blocks.3.mlp.0.weight 2 (1536, 384)\n",
            "model.decoder.layers.3.fc1.bias  ->  decoder.blocks.3.mlp.0.bias\n",
            "decoder.blocks.3.mlp.0.bias 1 (1536,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.fc2.weight  ->  decoder.blocks.3.mlp.2.weight\n",
            "decoder.blocks.3.mlp.2.weight 2 (384, 1536)\n",
            "model.decoder.layers.3.fc2.bias  ->  decoder.blocks.3.mlp.2.bias\n",
            "decoder.blocks.3.mlp.2.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.final_layer_norm.weight  ->  decoder.blocks.3.mlp_ln.weight\n",
            "decoder.blocks.3.mlp_ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layers.3.final_layer_norm.bias  ->  decoder.blocks.3.mlp_ln.bias\n",
            "decoder.blocks.3.mlp_ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layer_norm.weight  ->  decoder.ln.weight\n",
            "decoder.ln.weight 1 (384,)\n",
            "  Converting to float32\n",
            "model.decoder.layer_norm.bias  ->  decoder.ln.bias\n",
            "decoder.ln.bias 1 (384,)\n",
            "  Converting to float32\n",
            "Skipping proj_out.weight\n",
            "Done. Output file:  ggml/ggml-model.bin\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For smaller size and better performance, we can also quantize the GGML model:"
      ],
      "metadata": {
        "id": "UOdPpP1fMRr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd whisper.cpp && cmake -B build && cmake --build build --config Release\n",
        "!./whisper.cpp/build/bin/quantize ./ggml/ggml-clean.bin ./ggml/ggml-clean-q8_0.bin q8_0"
      ],
      "metadata": {
        "id": "xD4AX-4ZL8xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be83521-6894-4d53-abc4-0ebd0fc5d4fe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.10 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
            "  to tell CMake that the project requires at least <min> but has been updated\n",
            "  to work with policies introduced by <max> or earlier.\n",
            "\n",
            "\u001b[0m\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Configuring done (0.1s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/whisper.cpp/build\n",
            "[ 17%] Built target ggml-base\n",
            "[ 40%] Built target ggml-cpu\n",
            "[ 45%] Built target ggml\n",
            "[ 50%] Built target whisper\n",
            "[ 60%] Built target common\n",
            "[ 65%] Built target whisper-cli\n",
            "[ 70%] Built target whisper-bench\n",
            "[ 75%] Built target whisper-server\n",
            "[ 80%] Built target quantize\n",
            "[ 85%] Built target main\n",
            "[ 90%] Built target bench\n",
            "[ 95%] Built target stream\n",
            "[100%] Built target command\n",
            "whisper_model_quantize: loading model from './ggml/ggml-clean.bin'\n",
            "whisper_model_quantize: n_vocab       = 51865\n",
            "whisper_model_quantize: n_audio_ctx   = 1500\n",
            "whisper_model_quantize: n_audio_state = 384\n",
            "whisper_model_quantize: n_audio_head  = 6\n",
            "whisper_model_quantize: n_audio_layer = 4\n",
            "whisper_model_quantize: n_text_ctx    = 448\n",
            "whisper_model_quantize: n_text_state  = 384\n",
            "whisper_model_quantize: n_text_head   = 6\n",
            "whisper_model_quantize: n_text_layer  = 4\n",
            "whisper_model_quantize: n_mels        = 80\n",
            "whisper_model_quantize: ftype (src)   = 1\n",
            "whisper_model_quantize: qntvr (src)   = 0\n",
            "whisper_model_quantize: ftype (dst)   = 2007\n",
            "whisper_model_quantize: qntvr (dst)   = 2\n",
            "                                            encoder.conv1.weight - [    3,    80,   384], type =    f16 size =    0.176 MB\n",
            "                                              encoder.conv1.bias - [    1,   384,     1], type =    f32 size =    0.001 MB\n",
            "                                            encoder.conv2.weight - [    3,   384,   384], type =    f16 size =    0.844 MB\n",
            "                                              encoder.conv2.bias - [    1,   384,     1], type =    f32 size =    0.001 MB\n",
            "                                    encoder.positional_embedding - [  384,  1500,     1], type =    f32 size =    2.197 MB\n",
            "                                encoder.blocks.0.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              encoder.blocks.0.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.0.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              encoder.blocks.0.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.0.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                encoder.blocks.0.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  encoder.blocks.0.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 encoder.blocks.0.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.0.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.0.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.0.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   encoder.blocks.0.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.0.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  encoder.blocks.0.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    encoder.blocks.0.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                encoder.blocks.1.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              encoder.blocks.1.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.1.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              encoder.blocks.1.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.1.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                encoder.blocks.1.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  encoder.blocks.1.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 encoder.blocks.1.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.1.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.1.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.1.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   encoder.blocks.1.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.1.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  encoder.blocks.1.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    encoder.blocks.1.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                encoder.blocks.2.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              encoder.blocks.2.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.2.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              encoder.blocks.2.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.2.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                encoder.blocks.2.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  encoder.blocks.2.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 encoder.blocks.2.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.2.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.2.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.2.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   encoder.blocks.2.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.2.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  encoder.blocks.2.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    encoder.blocks.2.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                encoder.blocks.3.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              encoder.blocks.3.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.3.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              encoder.blocks.3.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                encoder.blocks.3.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                encoder.blocks.3.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  encoder.blocks.3.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 encoder.blocks.3.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.3.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   encoder.blocks.3.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.3.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   encoder.blocks.3.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     encoder.blocks.3.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  encoder.blocks.3.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    encoder.blocks.3.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                          encoder.ln_post.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                            encoder.ln_post.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  decoder.token_embedding.weight - [  384, 51865,     1], type =    f16 size =    75.97 MB ->    20.18 MB\n",
            "                                    decoder.positional_embedding - [  384,   448,     1], type =    f32 size =    0.656 MB\n",
            "                                decoder.blocks.0.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              decoder.blocks.0.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.0.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              decoder.blocks.0.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.0.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                decoder.blocks.0.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  decoder.blocks.0.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 decoder.blocks.0.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.0.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.0.cross_attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                        decoder.blocks.0.cross_attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.0.cross_attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                        decoder.blocks.0.cross_attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.0.cross_attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.0.cross_attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                            decoder.blocks.0.cross_attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                           decoder.blocks.0.cross_attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                             decoder.blocks.0.cross_attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.0.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.0.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   decoder.blocks.0.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.0.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  decoder.blocks.0.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    decoder.blocks.0.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                decoder.blocks.1.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              decoder.blocks.1.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.1.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              decoder.blocks.1.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.1.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                decoder.blocks.1.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  decoder.blocks.1.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 decoder.blocks.1.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.1.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.1.cross_attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                        decoder.blocks.1.cross_attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.1.cross_attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                        decoder.blocks.1.cross_attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.1.cross_attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.1.cross_attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                            decoder.blocks.1.cross_attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                           decoder.blocks.1.cross_attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                             decoder.blocks.1.cross_attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.1.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.1.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   decoder.blocks.1.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.1.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  decoder.blocks.1.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    decoder.blocks.1.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                decoder.blocks.2.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              decoder.blocks.2.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.2.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              decoder.blocks.2.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.2.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                decoder.blocks.2.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  decoder.blocks.2.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 decoder.blocks.2.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.2.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.2.cross_attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                        decoder.blocks.2.cross_attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.2.cross_attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                        decoder.blocks.2.cross_attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.2.cross_attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.2.cross_attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                            decoder.blocks.2.cross_attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                           decoder.blocks.2.cross_attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                             decoder.blocks.2.cross_attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.2.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.2.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   decoder.blocks.2.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.2.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  decoder.blocks.2.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    decoder.blocks.2.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                decoder.blocks.3.attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                              decoder.blocks.3.attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.3.attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                              decoder.blocks.3.attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                decoder.blocks.3.attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                decoder.blocks.3.attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                                  decoder.blocks.3.attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                 decoder.blocks.3.attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.3.attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.3.cross_attn.key.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                        decoder.blocks.3.cross_attn.value.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.3.cross_attn.value.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                        decoder.blocks.3.cross_attn.query.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                          decoder.blocks.3.cross_attn.query.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                          decoder.blocks.3.cross_attn.out.weight - [  384,   384,     1], type =    f16 size =     0.56 MB ->     0.15 MB\n",
            "                            decoder.blocks.3.cross_attn.out.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                           decoder.blocks.3.cross_attn_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                             decoder.blocks.3.cross_attn_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                   decoder.blocks.3.mlp.0.weight - [  384,  1536,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.3.mlp.0.bias - [ 1536,     1,     1], type =    f32 size =    0.006 MB\n",
            "                                   decoder.blocks.3.mlp.2.weight - [ 1536,   384,     1], type =    f16 size =     2.25 MB ->     0.60 MB\n",
            "                                     decoder.blocks.3.mlp.2.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                  decoder.blocks.3.mlp_ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                    decoder.blocks.3.mlp_ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                               decoder.ln.weight - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "                                                 decoder.ln.bias - [  384,     1,     1], type =    f32 size =    0.001 MB\n",
            "ggml_common_quantize_0: model size  =   144.05 MB\n",
            "ggml_common_quantize_0: quant size  =    40.97 MB | ftype = 7 (q8_0)\n",
            "\n",
            "main: quantize time =   465.14 ms\n",
            "main:    total time =   465.14 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's make sure that the `.ggml` model works. Start by downloading some test audio:"
      ],
      "metadata": {
        "id": "ZNH_WV-lMW9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./test-audio\n",
        "# Download the first chapter of Alice in Wonderland (in French)\n",
        "!wget -P ./test-audio/ https://www.archive.org/download/alice_au_pays_des_merveilles_1811_librivox/aliceaupays_01_carroll_128kb.mp3\n",
        "!wget -P ./test-audio/ https://www.archive.org/download/alice_in_wonderland_librivox/wonderland_ch_01.mp3\n",
        "# Convert it to a format that's understandable by whisper.cpp:\n",
        "# -t 30                 Take the first 30s\n",
        "# -i ...                Input path\n",
        "# -ar 16000             Sample rate of 16000 HZ\n",
        "# -ac 1                 1 audio channel\n",
        "# -codec:a pcm_s16le    Audio codec\n",
        "!ffmpeg -t 30 -i ./test-audio/aliceaupays_01_carroll_128kb.mp3 -ar 16000 -ac 1 -codec:a pcm_s16le ./test-audio/recording-fr.wav\n",
        "!ffmpeg -t 30 -i ./test-audio/wonderland_ch_01.mp3 -ar 16000 -ac 1 -codec:a pcm_s16le ./test-audio/recording-en.wav"
      ],
      "metadata": {
        "id": "YFcwkwhXMYKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a35b6d7-2fc7-47e5-86df-4daccaf6bf4c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./test-audio’: File exists\n",
            "--2025-02-26 16:43:23--  https://www.archive.org/download/alice_au_pays_des_merveilles_1811_librivox/aliceaupays_01_carroll_128kb.mp3\n",
            "Resolving www.archive.org (www.archive.org)... 207.241.224.2\n",
            "Connecting to www.archive.org (www.archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://archive.org/download/alice_au_pays_des_merveilles_1811_librivox/aliceaupays_01_carroll_128kb.mp3 [following]\n",
            "--2025-02-26 16:43:24--  https://archive.org/download/alice_au_pays_des_merveilles_1811_librivox/aliceaupays_01_carroll_128kb.mp3\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia803201.us.archive.org/25/items/alice_au_pays_des_merveilles_1811_librivox/aliceaupays_01_carroll_128kb.mp3 [following]\n",
            "--2025-02-26 16:43:24--  https://ia803201.us.archive.org/25/items/alice_au_pays_des_merveilles_1811_librivox/aliceaupays_01_carroll_128kb.mp3\n",
            "Resolving ia803201.us.archive.org (ia803201.us.archive.org)... 207.241.234.121\n",
            "Connecting to ia803201.us.archive.org (ia803201.us.archive.org)|207.241.234.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17754577 (17M) [audio/mpeg]\n",
            "Saving to: ‘./test-audio/aliceaupays_01_carroll_128kb.mp3.1’\n",
            "\n",
            "aliceaupays_01_carr 100%[===================>]  16.93M  10.6MB/s    in 1.6s    \n",
            "\n",
            "2025-02-26 16:43:27 (10.6 MB/s) - ‘./test-audio/aliceaupays_01_carroll_128kb.mp3.1’ saved [17754577/17754577]\n",
            "\n",
            "--2025-02-26 16:43:27--  https://www.archive.org/download/alice_in_wonderland_librivox/wonderland_ch_01.mp3\n",
            "Resolving www.archive.org (www.archive.org)... 207.241.224.2\n",
            "Connecting to www.archive.org (www.archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://archive.org/download/alice_in_wonderland_librivox/wonderland_ch_01.mp3 [following]\n",
            "--2025-02-26 16:43:27--  https://archive.org/download/alice_in_wonderland_librivox/wonderland_ch_01.mp3\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia903401.us.archive.org/8/items/alice_in_wonderland_librivox/wonderland_ch_01.mp3 [following]\n",
            "--2025-02-26 16:43:28--  https://ia903401.us.archive.org/8/items/alice_in_wonderland_librivox/wonderland_ch_01.mp3\n",
            "Resolving ia903401.us.archive.org (ia903401.us.archive.org)... 207.241.230.191\n",
            "Connecting to ia903401.us.archive.org (ia903401.us.archive.org)|207.241.230.191|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10249859 (9.8M) [audio/mpeg]\n",
            "Saving to: ‘./test-audio/wonderland_ch_01.mp3.1’\n",
            "\n",
            "wonderland_ch_01.mp 100%[===================>]   9.77M  5.96MB/s    in 1.6s    \n",
            "\n",
            "2025-02-26 16:43:31 (5.96 MB/s) - ‘./test-audio/wonderland_ch_01.mp3.1’ saved [10249859/10249859]\n",
            "\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mp3, from './test-audio/aliceaupays_01_carroll_128kb.mp3':\n",
            "  Metadata:\n",
            "    title           : 01 - Au Fond du Terrier\n",
            "    artist          : Lewis Carroll\n",
            "    album           : Aventures d'Alice au Pays des Merveilles\n",
            "    genre           : speech\n",
            "    track           : 2\n",
            "  Duration: 00:18:29.37, start: 0.025057, bitrate: 128 kb/s\n",
            "  Stream #0:0: Audio: mp3, 44100 Hz, mono, fltp, 128 kb/s\n",
            "    Metadata:\n",
            "      encoder         : LAME3.99r\n",
            "File './test-audio/recording-fr.wav' already exists. Overwrite? [y/N] ^C\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "\u001b[0;35m[mp3 @ 0x59ad5eabc040] \u001b[0m\u001b[0;33mEstimating duration from bitrate, this may be inaccurate\n",
            "\u001b[0mInput #0, mp3, from './test-audio/wonderland_ch_01.mp3':\n",
            "  Metadata:\n",
            "    title           : wonderland_ch_01\n",
            "    iTunNORM        :  0000024F 000001A7 00000B2D 0000079B 000975F7 000668CE 000059C5 00004487 000975E0 000975E0\n",
            "    artist          : Lewis Carroll\n",
            "    album           : Alice in Wonderland\n",
            "    genre           : Audio Book\n",
            "    comment         : LibriVox.org\n",
            "    track           : 1/12\n",
            "  Duration: 00:10:40.57, start: 0.000000, bitrate: 128 kb/s\n",
            "  Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 128 kb/s\n",
            "File './test-audio/recording-en.wav' already exists. Overwrite? [y/N] ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, use the `whisper-cli` command to transcribe the audio using our GGML model:"
      ],
      "metadata": {
        "id": "kg5hkQpyq1io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test converting the WAV file to text using the GGML file that we built\n",
        "!./whisper.cpp/build/bin/whisper-cli --language fr --no-timestamps -m ./ggml/ggml-clean.bin ./test-audio/recording-fr.wav\n",
        "!./whisper.cpp/build/bin/whisper-cli --language en --no-timestamps -m ./ggml/ggml-clean.bin ./test-audio/recording-en.wav"
      ],
      "metadata": {
        "id": "ur0wEF6SULr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b979ac09-b258-469d-f128-c033a40f1916"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whisper_init_from_file_with_params_no_state: loading model from './ggml/ggml-clean.bin'\n",
            "whisper_init_with_params_no_state: use gpu    = 1\n",
            "whisper_init_with_params_no_state: flash attn = 0\n",
            "whisper_init_with_params_no_state: gpu_device = 0\n",
            "whisper_init_with_params_no_state: dtw        = 0\n",
            "whisper_init_with_params_no_state: devices    = 1\n",
            "whisper_init_with_params_no_state: backends   = 1\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51865\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 384\n",
            "whisper_model_load: n_audio_head  = 6\n",
            "whisper_model_load: n_audio_layer = 4\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 384\n",
            "whisper_model_load: n_text_head   = 6\n",
            "whisper_model_load: n_text_layer  = 4\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: ftype         = 1\n",
            "whisper_model_load: qntvr         = 0\n",
            "whisper_model_load: type          = 1 (tiny)\n",
            "whisper_model_load: adding 1608 extra tokens\n",
            "whisper_model_load: n_langs       = 99\n",
            "whisper_model_load:      CPU total size =    77.11 MB\n",
            "whisper_model_load: model size    =   77.11 MB\n",
            "whisper_init_state: kv self size  =    3.15 MB\n",
            "whisper_init_state: kv cross size =    9.44 MB\n",
            "whisper_init_state: kv pad  size  =    2.36 MB\n",
            "whisper_init_state: compute buffer (conv)   =   13.19 MB\n",
            "whisper_init_state: compute buffer (encode) =   64.79 MB\n",
            "whisper_init_state: compute buffer (cross)  =    3.88 MB\n",
            "whisper_init_state: compute buffer (decode) =   95.89 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 0 | \n",
            "\n",
            "main: processing './test-audio/recording-fr.wav' (480000 samples, 30.0 sec), 2 threads, 1 processors, 5 beams + best of 5, lang = fr, task = transcribe, timestamps = 0 ...\n",
            "\n",
            "\n",
            " … J'appêtre premier deTORures d'Alice au pays d'Emerveille par Louis Carl. Praduit par Nab-Bué, c'est un regist Xiangозм-vox fait partie du domaine public. EnHelpré par Linda Olsen-Fightak, Los Angeles. Au fond du terrier, Alice, assise au prête s'assure.\n",
            "\n",
            "whisper_print_timings:     load time =    80.68 ms\n",
            "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
            "whisper_print_timings:      mel time =    65.73 ms\n",
            "whisper_print_timings:   sample time =   421.31 ms /   342 runs (    1.23 ms per run)\n",
            "whisper_print_timings:   encode time =  2371.49 ms /     1 runs ( 2371.49 ms per run)\n",
            "whisper_print_timings:   decode time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:   batchd time =  1163.00 ms /   341 runs (    3.41 ms per run)\n",
            "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:    total time =  4126.01 ms\n",
            "whisper_init_from_file_with_params_no_state: loading model from './ggml/ggml-clean.bin'\n",
            "whisper_init_with_params_no_state: use gpu    = 1\n",
            "whisper_init_with_params_no_state: flash attn = 0\n",
            "whisper_init_with_params_no_state: gpu_device = 0\n",
            "whisper_init_with_params_no_state: dtw        = 0\n",
            "whisper_init_with_params_no_state: devices    = 1\n",
            "whisper_init_with_params_no_state: backends   = 1\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51865\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 384\n",
            "whisper_model_load: n_audio_head  = 6\n",
            "whisper_model_load: n_audio_layer = 4\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 384\n",
            "whisper_model_load: n_text_head   = 6\n",
            "whisper_model_load: n_text_layer  = 4\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: ftype         = 1\n",
            "whisper_model_load: qntvr         = 0\n",
            "whisper_model_load: type          = 1 (tiny)\n",
            "whisper_model_load: adding 1608 extra tokens\n",
            "whisper_model_load: n_langs       = 99\n",
            "whisper_model_load:      CPU total size =    77.11 MB\n",
            "whisper_model_load: model size    =   77.11 MB\n",
            "whisper_init_state: kv self size  =    3.15 MB\n",
            "whisper_init_state: kv cross size =    9.44 MB\n",
            "whisper_init_state: kv pad  size  =    2.36 MB\n",
            "whisper_init_state: compute buffer (conv)   =   13.19 MB\n",
            "whisper_init_state: compute buffer (encode) =   64.79 MB\n",
            "whisper_init_state: compute buffer (cross)  =    3.88 MB\n",
            "whisper_init_state: compute buffer (decode) =   95.89 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 0 | \n",
            "\n",
            "main: processing './test-audio/recording-en.wav' (480000 samples, 30.0 sec), 2 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 0 ...\n",
            "\n",
            "\n",
            " This is a Librivox recording. All Librivox recordings are in the public domain for more information or to volunteer visit Librivox.org. Alice's adventures in Wonderland by Lewisadaki. Chapter 1, down the rabbit hole. Alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do. Once or twice, she peeped into the book her sister was reading, but had no pictures or conversations in it. And what's the use of a book?\n",
            "\n",
            "whisper_print_timings:     load time =    79.15 ms\n",
            "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
            "whisper_print_timings:      mel time =    65.01 ms\n",
            "whisper_print_timings:   sample time =   678.56 ms /   550 runs (    1.23 ms per run)\n",
            "whisper_print_timings:   encode time =  1647.40 ms /     1 runs ( 1647.40 ms per run)\n",
            "whisper_print_timings:   decode time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:   batchd time =  1796.61 ms /   549 runs (    3.27 ms per run)\n",
            "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:    total time =  4290.03 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare with the upstream model\n",
        "!mkdir ./ggml-upstream/\n",
        "!sh ./whisper.cpp/models/download-ggml-model.sh tiny ./ggml-upstream/\n",
        "!./whisper.cpp/build/bin/whisper-cli --language fr --no-timestamps -m ./ggml-upstream/ggml-tiny.bin ./test-audio/recording-fr.wav"
      ],
      "metadata": {
        "id": "kT2qwsdQL9sP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c035a881-f1e1-464f-9c65-473d1178c719"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘./ggml-upstream/’: File exists\n",
            "Downloading ggml model tiny from 'https://huggingface.co/ggerganov/whisper.cpp' ...\n",
            "Model tiny already exists. Skipping download.\n",
            "whisper_init_from_file_with_params_no_state: loading model from './ggml-upstream/ggml-tiny.bin'\n",
            "whisper_init_with_params_no_state: use gpu    = 1\n",
            "whisper_init_with_params_no_state: flash attn = 0\n",
            "whisper_init_with_params_no_state: gpu_device = 0\n",
            "whisper_init_with_params_no_state: dtw        = 0\n",
            "whisper_init_with_params_no_state: devices    = 1\n",
            "whisper_init_with_params_no_state: backends   = 1\n",
            "whisper_model_load: loading model\n",
            "whisper_model_load: n_vocab       = 51865\n",
            "whisper_model_load: n_audio_ctx   = 1500\n",
            "whisper_model_load: n_audio_state = 384\n",
            "whisper_model_load: n_audio_head  = 6\n",
            "whisper_model_load: n_audio_layer = 4\n",
            "whisper_model_load: n_text_ctx    = 448\n",
            "whisper_model_load: n_text_state  = 384\n",
            "whisper_model_load: n_text_head   = 6\n",
            "whisper_model_load: n_text_layer  = 4\n",
            "whisper_model_load: n_mels        = 80\n",
            "whisper_model_load: ftype         = 1\n",
            "whisper_model_load: qntvr         = 0\n",
            "whisper_model_load: type          = 1 (tiny)\n",
            "whisper_model_load: adding 1608 extra tokens\n",
            "whisper_model_load: n_langs       = 99\n",
            "whisper_model_load:      CPU total size =    77.11 MB\n",
            "whisper_model_load: model size    =   77.11 MB\n",
            "whisper_init_state: kv self size  =    3.15 MB\n",
            "whisper_init_state: kv cross size =    9.44 MB\n",
            "whisper_init_state: kv pad  size  =    2.36 MB\n",
            "whisper_init_state: compute buffer (conv)   =   13.19 MB\n",
            "whisper_init_state: compute buffer (encode) =   64.79 MB\n",
            "whisper_init_state: compute buffer (cross)  =    3.88 MB\n",
            "whisper_init_state: compute buffer (decode) =   95.89 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | COREML = 0 | OPENVINO = 0 | \n",
            "\n",
            "main: processing './test-audio/recording-fr.wav' (480000 samples, 30.0 sec), 2 threads, 1 processors, 5 beams + best of 5, lang = fr, task = transcribe, timestamps = 0 ...\n",
            "\n",
            "\n",
            " … J'appêtre premier de aventures d'Alice au pays d'Emerveille par Louis Carl. Praduit par Henri-Bué, c'est un registrement libre-vox fait partie du domaine public. Enregistré par Linda Olsen-Fightak, Los Angeles. Au fond du terrier, Alice, assise au prête s'assure.\n",
            "\n",
            "whisper_print_timings:     load time =    92.60 ms\n",
            "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
            "whisper_print_timings:      mel time =    63.42 ms\n",
            "whisper_print_timings:   sample time =   423.38 ms /   342 runs (    1.24 ms per run)\n",
            "whisper_print_timings:   encode time =  1653.52 ms /     1 runs ( 1653.52 ms per run)\n",
            "whisper_print_timings:   decode time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:   batchd time =  1268.41 ms /   341 runs (    3.72 ms per run)\n",
            "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
            "whisper_print_timings:    total time =  3525.13 ms\n"
          ]
        }
      ]
    }
  ]
}